{
  "hash": "001b86f603427591a8c4cc10b2cbf9e9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"07-linear-models\"\nauthor: \"Professor Shannon Ellis\"\ndate: \"2023-10-24\"\n\nformat:\n  html: \n    output-file: 07-linear-models.html\n    embed-resources: true\n  revealjs:\n    output-file: 07-linear-models-slides.html\n    slide-number: true\n    chalkboard: false \n    preview-links: auto\n    logo: images/cogs137-logo-hex.png\n    css: slides.css\n    footer: <https://cogs137.github.io/website/>\n    scrollable: true\n    embed-resources: true\n    execute:\n      echo: true\n      eval: true\n---\n\n\n# Linear Models {background-color=\"#92A86A\"}\n\n## Q&A {.smaller}\n\n> Q: I was just wondering if you could still provide those videos you talked about with like syntax stuff so that we can follow along. I would also appreciate if it was really through with like a walk through of how you got to each point\\\n> A: I like this suggestion, and I'll try to make these. No promises though. Just have to find magical time in the schedule to make them. Note that lectures will be the more detailed walk through with more time on your own to figure it out (rather than detailed walk through outside of class), so you will get the information...the time/location will just differ. But, I'll try to make some supplemental videos for those interested.\n\n> Q: Had a question about the modeling and the last bit showing how decreasing the alpha showed greater clustering at the bottom left corner. If the bottom left corner looks like 0 height and 0 width, how does that translate into the dimensions of an actual painting?\\\n> A: Good observation. The follow-up to this is...are there any paintings with zero width or zero height? And, if you dig in the data (i.e. `min(pp$Height_in, na.rm=TRUE)`), you'll see that there are some very small paintings, but that none are zero.\n\n> Q: If we want to built our own model, can we plot them with ggplot2? \\\n> A: Yup! [This post](https://stackoverflow.com/questions/44865508/using-ggplot2-to-plot-an-already-existing-linear-model) starts to get at that. It does so for a linear model, but the logic follows for other models.\n\n> Q: For the paintings dataset, how could we perform EDA on specific subject matter, like seeing how many portraits include Jesus as part of the subject? \\\n> A: Love this question. There is a whole field of natural language processing that would have sophisticated ways to analyze this. A simple first pass would be to, for example, filter for paintings that include \"Jesus\" in the `subject` column.\n\n> Q: I think the segmented bar plots based on proportion seem difficult to read. I'm not sure why we should be using this instead of the stacked plots? \\\n> A: Grouped bar blots are typically most quickly understood. Proportion stacked plots are then easiest to understand proportion across categories. Stacked plots of raw numbers take longer (for most) to understand and thus are often avoided, but like all viz, it depends on context and audience.\n\n> Q: In the last lecture, we talked about how segmented bar plots might not be the ideal choice for data visualization, but we still demonstrated them in today’s lecture. So, specifically in what cases should we choose segmented bar plots as a data analysis tool? \\\n> A: They can be helpful when the audience is familiar with them, but typically are most helpful when you want to display relative proportions rather than counts\n\n\n## Course Announcements {.incremental}\n\n**Due Dates**:\n\n-   **Lab 04** due Friday\n    -   Model Interpretations\n    -   Text, code, & viz all matter\n-   Lecture Participation survey \"due\" after class\n-   **HW02** due Monday (10/30; 11:59 PM)\n  - discuss displaying image in Markdown\n\n\n\n::: {.cell}\n\n:::\n\n\n\n## Agenda\n\n-   Linear Models\n    -   Quantitative Predictor\n    -   Categorical Predictor (2 & \\>2 levels)\n    -   residuals\n    -   data transformations\n\n## Suggested Reading\n\n-   R4DS Chapter 24: [Model Building](https://r4ds.had.co.nz/model-building.html)\n-   Introduction to Modern Statistics Chapter 7: [Linear Regression with a Single Predictor](https://openintro-ims.netlify.app/model-slr.html)\n\n# `tidymodels` {background-color=\"#92A86A\"}\n\n## Data: Paris Paintings\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp <- read_csv(\"https://raw.githubusercontent.com/COGS137/datasets/main/paris_paintings.csv\", \n               na = c(\"n/a\", \"\", \"NA\"))\n```\n:::\n\n\n-   Number of observations: 3393\n-   Number of variables: 61\n\n## Goal: Predict height from width\n\n$$\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}$$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/height-width-plot-1.png){width=2100}\n:::\n:::\n\n\n## `tidymodels`\n\n-   NOT a core `tidyverse` package\n-   follows the structure of a `tidyverse` package\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/07/tidymodels.png){width=98%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# should already be installed for you on datahub\nlibrary(tidymodels)\n```\n:::\n\n\n## Step 1: Specify model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n## Step 2: Set model fitting *engine*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") # lm: linear model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n## Step 3: Fit model & estimate parameters\n\n... using **formula syntax**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n```\n\n\n:::\n:::\n\n\n## A closer look at model output\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = Height_in ~ Width_in, data = data)\n\nCoefficients:\n(Intercept)     Width_in  \n     3.6214       0.7808  \n```\n\n\n:::\n:::\n\n\n$$\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}$$\n\n## A tidy look at model output\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ Width_in, data = pp) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.62    0.254        14.3 8.82e-45\n2 Width_in       0.781   0.00950      82.1 0       \n```\n\n\n:::\n:::\n\n\n$$\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}$$\n\n## Slope and intercept\n\n$$\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}$$\n\n. . .\n\n-   **Slope:** For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\n\n. . .\n\n-   **Intercept:** Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)\n\n. . .\n\n## Correlation does not imply causation\n\nRemember this when interpreting model coefficients\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/07/cell_phones.png){width=90%}\n:::\n:::\n\n\n::: aside\nSource: XKCD, [Cell phones](https://xkcd.com/925/)\n:::\n\n# Parameter Estimation {background-color=\"#92A86A\"}\n\n## Linear model with a single predictor\n\n-   We're interested in $\\beta_0$ (population parameter for the intercept) and $\\beta_1$ (population parameter for the slope) in the following model:\n\n$$\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}$$\n\n. . .\n\n-   Tough luck, you can't have them...\n\n. . .\n\n-   So we use sample statistics to estimate them:\n\n$$\\hat{y}_{i} = b_0 + b_1~x_{i}$$\n\n## Least squares regression\n\n-   The regression line minimizes the sum of squared residuals.\n\n. . .\n\n-   If $e_i = y_i - \\hat{y}_i$, then, the regression line minimizes $\\sum_{i = 1}^n e_i^2$.\n\n## Visualizing residuals\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/vis-res-1-1.png){width=2100}\n:::\n:::\n\n\n## Visualizing residuals (cont.)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/vis-res-2-1.png){width=2100}\n:::\n:::\n\n\n## Visualizing residuals (cont.)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/vis-res-3-1.png){width=2100}\n:::\n:::\n\n\n## Properties of least squares regression\n\n-   The regression line goes through the center of mass point, the coordinates corresponding to average $x$ and average $y$, $(\\bar{x}, \\bar{y})$:\n\n$$\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}$$\n\n. . .\n\n-   The slope has the same sign as the correlation coefficient: $b_1 = r \\frac{s_y}{s_x}$\n\n. . .\n\n-   The sum of the residuals is zero: $\\sum_{i = 1}^n e_i = 0$\n\n. . .\n\n-   The residuals and $x$ values are uncorrelated\n\n# Models with categorical explanatory variables {background-color=\"#92A86A\"}\n\n## Categorical predictor with 2 levels {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,393 × 3\n   name      Height_in landsALL\n   <chr>         <dbl>    <dbl>\n 1 L1764-2          37        0\n 2 L1764-3          18        0\n 3 L1764-4          13        1\n 4 L1764-5a         14        1\n 5 L1764-5b         14        1\n 6 L1764-6           7        0\n 7 L1764-7a          6        0\n 8 L1764-7b          6        0\n 9 L1764-8          15        0\n10 L1764-9a          9        0\n11 L1764-9b          9        0\n12 L1764-10a        16        1\n13 L1764-10b        16        1\n14 L1764-10c        16        1\n15 L1764-11         20        0\n16 L1764-12a        14        1\n17 L1764-12b        14        1\n18 L1764-13a        15        1\n19 L1764-13b        15        1\n20 L1764-14         37        0\n# ℹ 3,373 more rows\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n-   `landsALL = 0`: No landscape features\n-   `landsALL = 1`: Some landscape features\n:::\n:::\n\n## Height & landscape features {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_ht_lands <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ factor(landsALL), data = pp)\n\nm_ht_lands |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)          22.7      0.328      69.1 0       \n2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26\n```\n\n\n:::\n:::\n\n\n## Height & landscape features\n\n$$\\widehat{Height_{in}} = 22.7 - 5.645~landsALL$$\n\n-   **Slope:** Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\n    -   Compares baseline level (`landsALL = 0`) to the other level (`landsALL = 1`)\n-   **Intercept:** Paintings that don't have landscape features are expected, on average, to be 22.7 inches tall\n\n## Categorical predictor with \\>2 levels {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n-   school from which painting came (details in a few slides)\n:::\n:::\n\n## Relationship between height and school\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Height_in ~ school_pntg, data = pp) |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n```\n\n\n:::\n:::\n\n\n## Dummy variables\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n```\n\n\n:::\n:::\n\n\n-   When the categorical explanatory variable has many levels, they're encoded to **dummy variables**\n-   Each coefficient describes the expected difference between heights in that particular school compared to the baseline level\n\n## Categorical predictor with 3+ levels\n\n::: columns\n::: {.column width=\"70%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> school_pntg </th>\n   <th style=\"text-align:center;\"> D_FL </th>\n   <th style=\"text-align:center;\"> F </th>\n   <th style=\"text-align:center;\"> G </th>\n   <th style=\"text-align:center;\"> I </th>\n   <th style=\"text-align:center;\"> S </th>\n   <th style=\"text-align:center;\"> X </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> A </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> D/FL </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;\"> 1 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> F </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;\"> 1 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> G </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;\"> 1 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> I </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;\"> 1 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> S </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;\"> 1 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> X </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;\"> 0 </td>\n   <td style=\"text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\n::: {.column width=\"30%\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3,393 × 3\n   name      Height_in school_pntg\n   <chr>         <dbl> <chr>      \n 1 L1764-2          37 F          \n 2 L1764-3          18 I          \n 3 L1764-4          13 D/FL       \n 4 L1764-5a         14 F          \n 5 L1764-5b         14 F          \n 6 L1764-6           7 I          \n 7 L1764-7a          6 F          \n 8 L1764-7b          6 F          \n 9 L1764-8          15 I          \n10 L1764-9a          9 D/FL       \n11 L1764-9b          9 D/FL       \n12 L1764-10a        16 X          \n13 L1764-10b        16 X          \n14 L1764-10c        16 X          \n15 L1764-11         20 D/FL       \n16 L1764-12a        14 D/FL       \n17 L1764-12b        14 D/FL       \n18 L1764-13a        15 D/FL       \n19 L1764-13b        15 D/FL       \n20 L1764-14         37 F          \n# ℹ 3,373 more rows\n```\n\n\n:::\n:::\n\n:::\n:::\n\n## The linear model with multiple predictors\n\n-   Population model:\n\n$$ \\hat{y} = \\beta_0 + \\beta_1~x_1 + \\beta_2~x_2 + \\cdots + \\beta_k~x_k $$\n\n. . .\n\n-   Sample model that we use to estimate the population model:\n\n$$ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k $$\n\n## Relationship b/w height and school {.smaller}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        14.0       10.0     1.40  0.162  \n2 school_pntgD/FL     2.33      10.0     0.232 0.816  \n3 school_pntgF       10.2       10.0     1.02  0.309  \n4 school_pntgG        1.65      11.9     0.139 0.889  \n5 school_pntgI       10.3       10.0     1.02  0.306  \n6 school_pntgS       30.4       11.4     2.68  0.00744\n7 school_pntgX        2.87      10.3     0.279 0.780  \n```\n\n\n:::\n:::\n\n\n-   **Austrian school (A)** paintings are expected, on average, to be **14 inches** tall.\n-   **Dutch/Flemish school (D/FL)** paintings are expected, on average, to be **2.33 inches taller** than *Austrian school* paintings.\n-   **French school (F)** paintings are expected, on average, to be **10.2 inches taller** than *Austrian school* paintings.\n-   **German school (G)** paintings are expected, on average, to be **1.65 inches taller** than *Austrian school* paintings.\n-   **Italian school (I)** paintings are expected, on average, to be **10.3 inches taller** than *Austrian school* paintings.\n-   **Spanish school (S)** paintings are expected, on average, to be **30.4 inches taller** than *Austrian school* paintings.\n-   Paintings whose school is **unknown (X)** are expected, on average, to be **2.87 inches taller** than *Austrian school* paintings. \\]\n\n# Prediction with models {background-color=\"#92A86A\"}\n\n## Predict height from width\n\n[❓ On average, how tall are paintings that are 60 inches wide? $$\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$]{style=\"background-color: #ADD8E6\"}\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\n3.62 + 0.78 * 60\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 50.42\n```\n\n\n:::\n:::\n\n\n\"On average, we expect paintings that are 60 inches wide to be 50.42 inches high.\"\n\n**Warning:** We \"expect\" this to happen, but there will be some variability. (We'll learn about measuring the variability around the prediction later.)\n\n## Prediction vs. extrapolation\n\n[❓ On average, how tall are paintings that are 400 inches wide? $$\\widehat{Height_{in}} = 3.62 + 0.78~Width_{in}$$]{style=\"background-color: #ADD8E6\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/extrapolate-1.png){width=2100}\n:::\n:::\n\n\n## Watch out for extrapolation!\n\n> \"When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.\"[^1] <br> Stephen Colbert, April 6th, 2010\n\n[^1]: [Introduction to Modern Statistics.](https://openintro-ims.netlify.app/model-slr.html#extrapolation-is-treacherous) \"Extrapolation is treacherous.\"\n\n# Measuring model fit {background-color=\"#92A86A\"}\n\n## Measuring the strength of the fit\n\n-   The strength of the fit of a linear model is most commonly evaluated using $R^2$.\n\n-   It tells us what percent of variability in the response variable is explained by the model.\n\n-   The remainder of the variability is explained by variables not included in the model.\n\n-   $R^2$ is sometimes called the coefficient of determination.\n\n## Obtaining $R^2$ in R {.smaller}\n\n-   Height vs. width\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(m_ht_wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>\n1     0.683         0.683  8.30     6749.       0     1 -11083. 22173. 22191.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(m_ht_wt)$r.squared # extract R-squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6829468\n```\n\n\n:::\n:::\n\n\nRoughly 68% of the variability in heights of paintings can be explained by their widths.\n\n. . .\n\n-   Height vs. landscape features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(m_ht_lands)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03456724\n```\n\n\n:::\n:::\n\n\n# Exploring linearity {background-color=\"#92A86A\"}\n\n## Data: Paris Paintings\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/unnamed-chunk-18-1.png){width=2100}\n:::\n:::\n\n\n## Price vs. width\n\n[❓ Describe the relationship between price and width of paintings whose width is less than 100in.]{style=\"background-color: #ADD8E6\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/unnamed-chunk-19-1.png){width=2100}\n:::\n:::\n\n\n## Price vs. width {.smaller}\n\n[❓ Which plot shows a more linear relationship?]{style=\"background-color: #ADD8E6\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/unnamed-chunk-20-1.png){width=1500}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/unnamed-chunk-21-1.png){width=1500}\n:::\n:::\n\n:::\n:::\n\n## Price vs. width, residuals {.smaller}\n\n[❓ Which plot shows a residuals that are uncorrelated with predicted values from the model?]{style=\"background-color: #ADD8E6\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/unnamed-chunk-22-1.png){width=1500}\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/unnamed-chunk-23-1.png){width=1500}\n:::\n:::\n\n:::\n:::\n\n. . .\n\n[❓What's the unit of residuals?]{style=\"background-color: #ADD8E6\"}\n\n## Transforming the data\n\n-   We saw that `price` has a right-skewed distribution, and the relationship between price and width of painting is non-linear.\n\n. . .\n\n-   In these situations a transformation applied to the response variable may be useful.\n\n. . .\n\n-   In order to decide which transformation to use, we should examine the distribution of the response variable.\n\n. . .\n\n-   The extremely right skewed distribution suggests that a log transformation may be useful.\n    -   log = natural log, $ln$\n    -   Default base of the `log` function in R is the natural log: <br> `log(x, base = exp(1))`\n\n## Logged price vs. width\n\n[❓ How do we interpret the slope of this model?]{style=\"background-color: #ADD8E6\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-linear-models_files/figure-html/unnamed-chunk-24-1.png){width=2100}\n:::\n:::\n\n\n## Interpreting models with log transformation {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_lprice_wt <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(log(price) ~ Width_in, data = pp_wt_lt_100)\n\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019\n```\n\n\n:::\n:::\n\n\n## Linear model with log transformation\n\n$$ \\widehat{log(price)} = 4.67 + 0.02 Width $$\n\n. . .\n\n-   For each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.02 livres.\n\n. . .\n\n-   which is not a very useful statement...\n\n## Working with logs\n\n-   Subtraction and logs: $log(a) − log(b) = log(a / b)$\n\n. . .\n\n-   Natural logarithm: $e^{log(x)} = x$\n\n. . .\n\n-   We can use these identities to \"undo\" the log transformation\n\n## Interpreting models with log transformation\n\nThe slope coefficient for the log transformed model is 0.02, meaning the log price difference between paintings whose widths are one inch apart is predicted to be 0.02 log livres.\n\n. . .\n\n$$ log(\\text{price for width x+1}) - log(\\text{price for width x}) = 0.02 $$\n\n. . .\n\n$$ log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right) = 0.02 $$\n\n. . .\n\n$$ e^{log\\left(\\frac{\\text{price for width x+1}}{\\text{price for width x}}\\right)} = e^{0.02} $$\n\n. . .\n\n$$ \\frac{\\text{price for width x+1}}{\\text{price for width x}} \\approx 1.02 $$\n\n. . .\n\nFor each additional inch the painting is wider, the price of the painting is expected to be higher, on average, by a factor of 1.02.\n\n## Shortcuts in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(estimate, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    4.67 \n2 Width_in       0.019\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm_lprice_wt |>\n  tidy() |>\n  select(term, estimate) |>\n  mutate(estimate = round(exp(estimate), 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   107.  \n2 Width_in        1.02\n```\n\n\n:::\n:::\n\n\n## Recap: Log Transformations {.smaller}\n\n-   Non-constant variance is one of the most common model violations, however it is usually fixable by transforming the response (y) variable.\n\n. . .\n\n-   The most common transformation when the response variable is right skewed is the log transform: $log(y)$, especially useful when the response variable is (extremely) right skewed.\n\n. . .\n\n-   This transformation is also useful for variance stabilization.\n\n. . .\n\n-   When using a log transformation on the response variable the interpretation of the slope changes: *\"For each unit increase in x, y is expected on average to be higher/lower* <br> by a factor of $e^{b_1}$.\"\n\n. . .\n\n-   Another useful transformation is the square root: $\\sqrt{y}$, especially useful when the response variable is counts.\n\n## Aside: when $y = 0$\n\nIn some cases the value of the response variable might be 0, and\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -Inf\n```\n\n\n:::\n:::\n\n\n. . .\n\nThe trick is to add a very small number to the value of the response variable for these cases so that the `log` function can still be applied:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog(0 + 0.00001)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -11.51293\n```\n\n\n:::\n:::\n\n\n## Recap {.smaller background-color=\"#92A86A\"}\n\n-   Can I carry out linear regression using the `tidymodels` approach?\n-   Can I interpret and explain the results from a linear model with a single predictor?\n-   Do I understand the limitations of modelling data w/ linear regression?\n-   Can I describe and implement the use of a dummy variable in linear regression?\n-   Can I determine when logistic transformation may be appropriate? Can I interpret these results?\n\n\n",
    "supporting": [
      "07-linear-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}