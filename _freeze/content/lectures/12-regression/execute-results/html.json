{
  "hash": "34f0b7436c9f154b3d4305514c850837",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"12-regression\"\nauthor: \"Professor Shannon Ellis\"\ndate: \"2024-11-14\"\n\nformat:\n  html: \n    output-file: 12-regression.html\n  revealjs:\n    output-file: 12-regression-slides.html\n    css: slides.css\n---\n\n::: {.cell}\n\n:::\n\n\n## Q&A {.smaller}\n\n> Q: kind of zoned out. When is it appropriate to use inference or machine learning? \\\n> A: We use inference when we want to quantify the relationship between features and outcome to understand relationships. We use machine learning when we want to make predictions on outcomes from features. \n\n> Q: I am confused why in EDA, we have been finding the correlation between variables that seem to be measuring the same thing (such as graphs for log distance to primary or primary and secondary road). Why are these helpful to us?\\\n> A: We are ultimately trying to build a model to predict PM2.5 values...but we have a lot of features from which to choose, many which measure very siimilar things. We're using EDA to get a sense of what data we have and the relationships between variables, before ultimately deciding what to include in our ML model.\n\n## Course Announcements\n\n**Due Dates**\n\n-   🧪 **Lab 06** due Thursday\n-   📋 Lecture Participation survey \"due\" after class\n\n**Notes**\n\n- lab05 scores posted; cs01 scores/feedback posted by EOD\n- lab07 now available | longer than typical labs\n- Reminder to add any CS02 EDA you want to share with your classmates to the [padlet from last class](https://padlet.com/shannon0ellis/cs02-eda-9fn2bbxjm3qfc3d3)\n\n## Agenda\n\n- Simple linear regression (review)\n- Multiple linear regression\n- Model Comparison\n- Backward Selection\n\n## Suggested Reading\n\n- [IMS Ch7: Linear regression with a single predictor](https://openintro-ims.netlify.app/model-slr)\n- [IMS Ch8: Linear regression with multiple predictors](https://openintro-ims.netlify.app/model-mlr)\n\n# Linear Regression {background-color=\"#92A86A\"}\n\n## Setup\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(olsrr) # will need to be installed\n```\n:::\n\n\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm <- read_csv(\"OCS_data/data/raw/pm25_data.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 876 Columns: 50\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): state, county, city\ndbl (47): id, value, fips, lat, lon, CMAQ, zcta, zcta_area, zcta_pop, imp_a5...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n## Starting Question\n\n> What is the relationship between the EPA's CMAQ and PM2.5 values?\n\nReminder: Community Multiscale Air Quality (CMAQ) is the EPA's air pollution model. The data do NOT use PM2.5 measures\n\n## Quick Look\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pm, aes(x=CMAQ, y=value)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](12-regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n# Single predictor {background-color=\"#92A86A\"}\n\n## Regression model\n\n$$ \\hat{y} = b_0 + b_1~x_1 $$\n\n- $\\hat{y}$ | outcome\n- $x_1$ | predictor\n- $b_0$ | y-intercept\n- $b_1$ | slope (effect size)\n\n## Step 1: Specify model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n## Step 2: Set model fitting *engine*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_mod <- linear_reg() |>\n  set_engine(\"lm\") # lm: linear model\n```\n:::\n\n\n## Step 3: Fit model & estimate parameters\n\n... using **formula syntax**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit model\nmod_cmaq_value <- lin_mod |>\n  fit(value ~ CMAQ, data = pm) \n\n# display results\nmod_cmaq_value |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    7.40     0.232       31.9 1.81e-148\n2 CMAQ           0.405    0.0260      15.6 1.83e- 48\n```\n\n\n:::\n:::\n\n\n. . . \n\n$$\\widehat{PM2.5}_{i} = 7.40 + 0.405 \\times CMAQ_{i}$$\n\n## Visualizing the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = pm, aes(x = CMAQ, y = value)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", fullrange = TRUE, color = \"#8E2C90\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](12-regression_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Slope and intercept\n\n$$\\widehat{PM2.5}_{i} = 7.40 + 0.405 \\times CMAQ_{i}$$\n\n. . . \n\n-   **Slope:** For each one unit increase in CMAQ, we expect PM2.5 to increase, on average, by 0.405 ug/m^3.\n\n. . . \n\n-   **Intercept:** Monitors in areas with a CMAQ of zero are expected to have PM2.5 values of 7.4 ug/m^3, on average. (Recall: WHO exposure guideline is < 10 ug/m^3 on average annually for PM2.5)\n\n\n## Model Understanding\n\n$$\\widehat{PM2.5}_{i} = 7.40 + 0.405 \\times CMAQ_{i}$$\n\n[❓ What would we expect the PM2.5 value to be in an area with a CMAQ of 2?]{style=\"background-color: #ADD8E6\"}\n\n## ❗️Prediction vs. extrapolation\n\n[❓ What would we expect the PM2.5 value to be in an area with a CMAQ of 100? $$\\widehat{PM2.5}_{i} = 7.40 + 0.405 \\times 100$$\n]{style=\"background-color: #ADD8E6\"}\n\n. . . \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-regression_files/figure-html/extrapolate-1.png){width=672}\n:::\n:::\n\n\n\n## Measuring the strength of the fit\n\n-   The strength of the fit of a linear model is most commonly evaluated using $R^2$.\n\n-   It tells us what percent of variability in the response variable is explained by the model.\n\n-   The remainder of the variability is explained by variables not included in the model.\n\n-   $R^2$ is sometimes called the coefficient of determination.\n\n## Obtaining $R^2$ in R {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(mod_cmaq_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.217         0.216  2.29      243. 1.83e-48     1 -1966. 3939. 3953.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(mod_cmaq_value)$r.squared # extract R-squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2172969\n```\n\n\n:::\n:::\n\n\nRoughly 21.7% of the variability in PM2.5 values can be explained by CMAQ.\n\n. . . \n\n...suggests that we can do better to explain the variance in PM2.5 values\n\n# Multiple predictors (MLR) {background-color=\"#92A86A\"}\n\n## MLR \n\n-   Sample model that we use to estimate the population model:\n\n$$ \\hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \\cdots + b_k~x_k $$\n\n## Updated question\n\n> What is the relationship between CMAQ & population density (predictors) and  and PM2.5 values (outcome)?\n\n-   Response variable: `value` (PM2.5)\n-   Explanatory variables: `CMAQ` and Population Density hi/low\n\n## Creating a low/high population density variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm <- pm |> \n  mutate(popdens = case_when(\n    popdens_zcta >= median(pm$popdens_zcta) ~ \"hi\",\n    popdens_zcta < median(pm$popdens_zcta) ~ \"low\"          \n  ))\n```\n:::\n\n\n[❓ What is this accomplishing?]{style=\"background-color: #ADD8E6\"}\n\n## PM2.5 & Poulation density\n\n::: panel-tabset\n### Plot\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-regression_files/figure-html/unnamed-chunk-11-1.png){width=100%}\n:::\n:::\n\n\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = pm, aes(x = value, fill = popdens)) +\n  geom_histogram(binwidth = 2.5) + \n  facet_grid(popdens ~ .) +\n  scale_fill_manual(values = c(\"#071381\", \"#E48957\")) +\n  guides(fill = \"none\") +\n  labs(x = \"PM2.5\", y = NULL) \n```\n:::\n\n:::\n\n## Two ways to model\n\n:::incremental\n-   **Main effects:** Assuming relationship between CMAQ and PM2.5 **does not vary** by whether it's a low or high population density monitor.\n-   **Interaction effects:** Assuming relationship between CMAQ and PM2. **varies** by whether or not it's a low or high population density monitor.\n:::\n\n## Interacting explanatory variables\n\n-   Including an interaction effect in the model allows for different slopes, i.e. nonparallel lines.\n-   This implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.\n-   This can be accomplished by adding an interaction variable: the product of two explanatory variables.\n\n## Two ways to model {.smaller}\n\n::: columns\n::: {.column width=\"30%\"}\n-   **Main effects:** Assuming relationship between CMAQ and PM2.5 **does not vary** by whether it's a low or high population density monitor.\n-   **Interaction effects:** Assuming relationship between CMAQ and PM2. **varies** by whether or not it's a low or high population density monitor.\n:::\n\n::: {.column width=\"70%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-regression_files/figure-html/pp-main-int-viz-1.png){width=800%}\n:::\n:::\n\n:::\n:::\n\n. . . \n\n[❓ Which does your intuition/knowledge of the data suggest is more appropriate?]{style=\"background-color: #ADD8E6\"}\n\nPut a <font color=\"#32cb31\">green</font> sticky if you think main; <font color=\"#ff65a3\">pink</font> if you think interaction.\n\n## Fit model with main effects {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm_main_fit <- lin_mod |>\n  fit(value ~ CMAQ + popdens, data = pm)\n\npm_main_fit |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    7.70     0.272      28.3  1.02e-125\n2 CMAQ           0.389    0.0270     14.4  1.89e- 42\n3 popdenslow    -0.345    0.160      -2.15 3.17e-  2\n```\n\n\n:::\n:::\n\n\n. . . \n\n$$\\widehat{PM2.5} = 7.70 + 0.389 \\times CMAQ - 0.345 \\times popdens$$\n\n. . . \n\n[❓ How do we interpret this model?]{style=\"background-color: #ADD8E6\"}\n\n## Solving the model\n\n-   High-population density: Plug in 0 for `popdens`\n\n$$\\widehat{PM2.5} = 7.70 + 0.389 \\times CMAQ - 0.345 \\times 0$$\\\n$= 7.70 + 0.389 \\times CMAQ$\n\n. . . \n\n-   Low-population density: Plug in 1 for `popdens`\n\n$$\\widehat{PM2.5} = 7.70 + 0.389 \\times CMAQ - 0.345 \\times 1$$\\\n$= 7.355 + 0.389 \\times CMAQ$\n\n## Visualizing main effects {.smaller}\n\n::: columns\n::: {.column width=\"40%\"}\n-   **Same slope:** *Rate of change* in PM2.5 as CMAQ increases does not vary between low- and high-population density monitor areas.\n-   **Different intercept:** Areas of low density have consistently lower PM2.5 values relative to high-population density areas\n:::\n\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-regression_files/figure-html/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n:::\n:::\n\n## Interpreting main effects {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm_main_fit |>\n  tidy() |> \n  mutate(exp_estimate = exp(estimate)) |>\n  select(term, estimate, exp_estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  term        estimate exp_estimate\n  <chr>          <dbl>        <dbl>\n1 (Intercept)    7.70      2217.   \n2 CMAQ           0.389        1.48 \n3 popdenslow    -0.345        0.708\n```\n\n\n:::\n:::\n\n\n::: incremental\n-   All else held constant, for each 1 unit increase in CMAQ, PM2.5 would expect to increase by 0.389.\n-   All else held constant, areas of low density have PM2.5 values, on average, that are 0.345 *lower* than in high density areas\n-   PM2.5 values in high-density areas with a CMAQ of zero, would expect to have a PM2.5 value of 7.7.\n:::\n\n\n## Interaction: `CMAQ * popdens` {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-regression_files/figure-html/unnamed-chunk-13-1.png){width=80%}\n:::\n:::\n\n\n## Fit model with interaction effects {.smaller}\n\n-   Response variable: `value` (PM2.5)\n-   Explanatory variables: `CMAQ`, `popdens`, and their interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm_int_fit <- lin_mod |>\n  fit(value ~ CMAQ * popdens, data = pm)\n```\n:::\n\n\n## Linear model with interaction effects {.smaller}\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        8.35     0.381      21.9  4.63e-85\n2 CMAQ               0.319    0.0396      8.05 2.69e-15\n3 popdenslow        -1.45     0.485      -3.00 2.79e- 3\n4 CMAQ:popdenslow    0.131    0.0540      2.42 1.56e- 2\n```\n\n\n:::\n:::\n\n\n$$\\widehat{PM2.5} = 8.35 + 0.32 \\times CMAQ - 1.45 \\times popdens + 0.13 \\times CMAQ * popdens$$\n\n## Interpretation of interaction effects {.smaller}\n\n::: incremental\n-   Rate of change in PM2.5 as CMAQ increases varies depending upon PopDens (different slopes & intercepts)\n:::\n\n. . . \n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](12-regression_files/figure-html/viz-interaction-effects2-1.png){width=100%}\n:::\n:::\n\n:::\n\n\n# Comparing models {background-color=\"#92A86A\"}\n\n## R-squared {.small}\n\n\n-   $R^2$ is the percentage of variability in the response variable explained by the regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(mod_cmaq_value)$r.squared #single predictor\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2172969\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(pm_main_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2214237\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(pm_int_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2266283\n```\n\n\n:::\n:::\n\n\n. . . \n\n-   The model with interactions has a slightly higher $R^2$.\n\n. . . \n\n-   However using $R^2$ for model selection in models with multiple explanatory variables is not a good idea as $R^2$ increases when **any** variable is added to the model.\n\n## Adjusted R-squared {.small}\n\n\nAdjusted R-squared adjusts for number of terms in the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(pm_main_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.21964\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(pm_int_fit)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2239676\n```\n\n\n:::\n:::\n\n\nIt appears that adding the interaction actually increased adjusted $R^2$, so we should indeed use the model with the interactions.\n\n\n## In pursuit of Occam's razor {.smaller}\n\n::: incremental\n-   Occam's Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.\n\n-   Model selection follows this principle.\n\n-   We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.\n\n-   In other words, we prefer the simplest best model, i.e. **parsimonious** model.\n:::\n\n# Backward selection {background-color=\"#92A86A\"}\n\n## Backward selection {.small}\n\n\n::: {.small}\nFor this demo, we'll ignore interaction effects...and just model main effects:\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm_full <-  lin_mod |>\n  fit(value ~ ., data=pm) \n```\n:::\n\n\n-   $R^2$ (full): 0.9125688\n\n. . . \n\n**Remove `zcta_area`**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npm_noarea <-  lin_mod |>\n  fit(value ~ . -zcta_area, data=pm) \n\nglance(pm_noarea)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9134989\n```\n\n\n:::\n:::\n\n\n. . . \n\n-   $R^2$ (full): 0.9126\n-   $R^2$ (no `zcta_area`): 0.9135\n\n...*Increased* improved variance explained, so remove variable\n\n. . . \n\n... continue to remove one by one until max variance explained is achieved. But, that process is tedious.\n\n## Other approach: `olsrr` {.small}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# requires package installation: \n# install.packages(\"olsrr\")\nlibrary(olsrr)\n```\n:::\n\n\n. . . \n\n**Step 1: Fit full model (w/o `tidymodels`)**\n\n::: {.smallest}\nNote: I'm only fitting a handful of variables to demo how it works. Theoretically, you would fit the full model and compare all combinations.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the model (not using tidymodels)\nmod <- lm(value ~ log_pri_length_25000 + log_prisec_length_15000 +\n          log_nei_2008_pm25_sum_15000 , data=pm)\n```\n:::\n\n\n. . . \n\n**Step 2: Determine which variables to remove**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_step_backward_p(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                                 Elimination Summary                                  \n-------------------------------------------------------------------------------------\n        Variable                              Adj.                                       \nStep          Removed           R-Square    R-Square     C(p)        AIC        RMSE     \n-------------------------------------------------------------------------------------\n   1    log_pri_length_25000      0.1643      0.1624    2.3628    3998.1896    2.3638    \n-------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n...specifies that `log_pri_length_25000` should be removed\n\n. . . \n\n**Step 2 (alternate): Compare all possible models...**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_step_all_possible(mod) |>\n  arrange(desc(adjr))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Index N\n1     4 2\n2     7 3\n3     5 2\n4     1 1\n5     2 1\n6     6 2\n7     3 1\n                                                                Predictors\n1                      log_prisec_length_15000 log_nei_2008_pm25_sum_15000\n2 log_pri_length_25000 log_prisec_length_15000 log_nei_2008_pm25_sum_15000\n3                         log_pri_length_25000 log_nei_2008_pm25_sum_15000\n4                                              log_nei_2008_pm25_sum_15000\n5                                                  log_prisec_length_15000\n6                             log_pri_length_25000 log_prisec_length_15000\n7                                                     log_pri_length_25000\n    R-Square Adj. R-Square Mallow's Cp\n1 0.16433852    0.16242406    2.362832\n2 0.16468608    0.16181230    4.000000\n3 0.13894927    0.13697665   28.867142\n4 0.12833737    0.12734004   37.945112\n5 0.12004781    0.11904100   46.598742\n6 0.12047519    0.11846024   48.152590\n7 0.06827606    0.06721001  100.644250\n```\n\n\n:::\n:::\n\n\n## On the full model\n\nThis will take a while to run: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_full <- lm(value ~ ., data=pm)\nols_step_backward_p(mod_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                                      Elimination Summary                                       \n-----------------------------------------------------------------------------------------------\n        Variable                                     Adj.                                          \nStep              Removed              R-Square    R-Square      C(p)          AIC        RMSE     \n-----------------------------------------------------------------------------------------------\n   1    zcta_area                        0.9907      0.9135    -686.9999    2393.0696    0.7597    \n   2    imp_a10000                       0.9907      0.9144    -688.9885    2391.1777    0.7557    \n   3    imp_a15000                       0.9907      0.9153    -690.9804    2389.2532    0.7518    \n   4    popdens                          0.9907      0.9161    -692.9669    2387.3804    0.7479    \n   5    lon                              0.9907       0.917    -694.9512    2385.5282    0.7442    \n   6    log_nei_2008_pm25_sum_25000      0.9907      0.9178    -696.9317    2383.7123    0.7405    \n   7    log_pri_length_5000              0.9907      0.9186    -698.8475    2382.5047    0.7371    \n   8    pov                              0.9907      0.9193    -700.7333    2381.5780    0.7339    \n   9    log_dist_to_prisec               0.9907      0.9199    -702.5666    2381.1422    0.7310    \n  10    log_prisec_length_5000           0.9906      0.9205    -704.3919    2380.7786    0.7281    \n  11    imp_a5000                        0.9906      0.9212    -706.2355    2380.2407    0.7252    \n  12    id                               0.9906      0.9216    -707.8749    2381.6028    0.7231    \n  13    log_prisec_length_1000           0.9906       0.922    -709.4443    2383.6011    0.7213    \n  14    county_area                      0.9905      0.9224    -710.9958    2385.7466    0.7196    \n  15    popdens_county                   0.9904      0.9226    -712.3595    2389.5945    0.7187    \n  16    county_pop                       0.9904      0.9226    -713.5295    2395.1638    0.7185    \n  17    log_prisec_length_10000          0.9903      0.9226    -714.6346    2401.2525    0.7185    \n  18    log_pri_length_10000             0.9902      0.9229    -716.1117    2403.9439    0.7172    \n  19    log_pri_length_15000             0.9901       0.923    -717.4012    2408.2793    0.7166    \n-----------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n. . . \n\n[❓ How does this maybe inform our ultimate goal (building a prediction model)?]{style=\"background-color: #ADD8E6\"}\n\n\n## Recap {.smaller background-color=\"#92A86A\"}\n\n-   Can you model and interpret linear models with multiple predictors?\n-   Can you explain the difference in a model with main effects vs. interaction effects?\n-   Can you compare different models and determine how to proceed?\n-   Can you carry out and explain backward selection?\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}