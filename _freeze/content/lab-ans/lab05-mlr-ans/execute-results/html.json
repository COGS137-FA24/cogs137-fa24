{
  "hash": "002ae6fafa7c88c39cf50d3bfeb9b646",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab05: Modelling course evaluations, Pt 2 - Answers\"\nauthor: \"Sean Trott\"\noutput: \n  html: \n    embed-resources: true\n    highlight: pygments\n    preview-links: auto\n---\n\n\n## Getting Started\n\n### Load packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\n### Data\n\n### Exercise 1\n\nFirst, we read in the data from the appropriate directory.\n\nWe also recreate the `bty_avg` column as we did in Lab 4, which is just the mean of beauty ratings from each of the relevant groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevals = read_csv(\"data/evals-mod.csv\")\nevals <- evals |>\n  rowwise() |>\n  mutate(bty_avg = mean( c( bty_f1lower, bty_f1upper,\n                            bty_f2upper, bty_m1lower,\n                            bty_m1upper, bty_m2upper) )) |>\n  ungroup()\n```\n:::\n\n\n## Part 1\n\n### Exercise 2\n\nAs in Lab 4, we fit a model `m_bty` predicting `score` from `bty_avg`.\n\nThe linear equation would be written as follows:\n\n$\\hat{y} = 3.88 + 0.07*X$\n\nAdditionally, this model has an $R^2$ of 0.035 and an adjusted $R^2$ of 0.0329.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_bty <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg, data = evals)\n\nm_bty |>\n  tidy() |>\n  select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.88  \n2 bty_avg       0.0666\n```\n\n\n:::\n\n```{.r .cell-code}\nm_bty |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0350        0.0329 0.535      16.7 0.0000508     1  -366.  738.  751.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n## Part 2\n\n### Exercise 3\n\n$\\hat{y} = 3.75 + 0.07X_1 + 0.17X_2$\n\nWhere $X_1$ is `bty_avg` and $X_2$ corresponds to a binary variable that takes on the value of `0` for female professors and `1` for male professors.\n\nThis model has an $R^2$ of 0.059 and an adjusted $R^2$ of 0.055.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_bty_gen <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + gender, data = evals)\n\nm_bty_gen |>\n  tidy() |>\n  select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.75  \n2 bty_avg       0.0742\n3 gendermale    0.172 \n```\n\n\n:::\n\n```{.r .cell-code}\nm_bty_gen |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl>\n1    0.0591        0.0550 0.529      14.5 0.000000818     2  -360.  729.  745.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n### Exercise 4\n\nThe intercept means: *female* teachers with an average beauty score of 0 are predicted to have a `score` of 3.75.\n\nThe slope for `bty_avg` means that for every 1-unit increase in `bty_avg`, teachers are predicted to have an increase in their `score` (relative to the intercept) of approximately 0.07. This is true for both male and female teachers, but of course, the predictions for male teachers are *also* affected by the slope of `gender`, which means: holding `bty_avg` constant, male teachers are predicted to have a higher `score` (relative to the intercept) by approximately 0.172.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_bty_gen |>\n  tidy() |>\n  select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)   3.75  \n2 bty_avg       0.0742\n3 gendermale    0.172 \n```\n\n\n:::\n:::\n\n\n### Exercise 5\n\nThis model has an $R^2$ of 0.059, which means that the model explains about 5.9% of the varaince in `score`.\n\n### Exercise 6\n\nFor male professors only, the intercept term can be added to the slope coefficient for $X_2$ (`gender`), giving us a simplified expression:\n\n$\\hat{y} = 3.92 + 0.07X_1$\n\n### Exercise 7\n\nHolding `bty_avg` constant, male professors are predicted to have a `score` that's about 0.172 higher than female professors. Thus, according to the model, male professors with equivalent beauty ratings have higher scores.\n\n### Exercise 8\n\nThe adjusted $R^2$ for the larger model (`m_bty_gen`) is 0.055, whereas it is 0.0329 for the smaller model (`m_bty`). The difference between these values is 0.0221.\n\nThis can be interpreted as follows: adding `gender` to a model explains an additional 2.21% of variance in `score` beyond the variance already explained by `bty_avg`.\n\n### Exercise 9\n\nThe slope for `bty_avg` in `m_bty` is 0.067. The slope for `bty_avg` in `m_bty_gen` is 0.074.\n\nThe short answer is yes: adding `gender` has changed the slope for `bty_avg`. Specifically, it has *increased* the steepness of this slope: the predicted increase in `score` for each 1-unit increase in `bty_avg` is higher in a model that's also equipped with `gender` than in a model without `gender`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_bty |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   3.88      0.0761     51.0  1.56e-191\n2 bty_avg       0.0666    0.0163      4.09 5.08e-  5\n```\n\n\n:::\n\n```{.r .cell-code}\nm_bty_gen |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   3.75      0.0847     44.3  6.23e-168\n2 bty_avg       0.0742    0.0163      4.56 6.48e-  6\n3 gendermale    0.172     0.0502      3.43 6.52e-  4\n```\n\n\n:::\n:::\n\n\n### Exercise 10\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_bty_rank <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + rank, data = evals)\n\nm_bty_rank |>\n  tidy() |>\n  select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  term             estimate\n  <chr>               <dbl>\n1 (Intercept)        3.98  \n2 bty_avg            0.0678\n3 ranktenure track  -0.161 \n4 ranktenured       -0.126 \n```\n\n\n:::\n:::\n\n\n$\\hat{y} = 3.98 + 0.068X_1 - 0.16X_2 - 0.13X_3$\n\nWhere $X_1$ is `bty_avg`, $X_2$ = 1 for `tenure track` professors (and 0 for all else), and $X_3$ = 1 for `tenured` professors (and 0 for all else).\n\nThe intercept can be interpreted as follows: `teaching` professors with a `bty_avg` of 0 are predicted to have a `score` of 3.98.\n\nFor each 1-unit increase in `bty_avg`, all professors should have an increase in `score` by 0.068. The critical difference is simply what else is added to this increase. For `teaching` professors, this increase is considered relative only to the intercept. However, `tenure track` professors are subject to a *decrease* of 0.16 in their predicted `score`, and `tenured` professors are also subject to a *decrease* of 0.13 in their predicted `score`.\n\n## Part 3\n\n### Exercise 11\n\nPersonally, I would expect either `cls_did_eval` or `cls_perc_eval` to be the worst predictors. All the other variables seem more likely to correlate with `score`, either because of potential societal biases (like `age` or `gender`), or because of how they might correlate with other factors that could make a class a better or worse experience for students, on average (e.g., `cls_students`).\n\nOf course, it's possible that the proportion of students who fill out the survey (`cls_perc_eval`) is correlated with `score`: maybe people are more motivated to fill out the evaluations for really great professors. This suggests that `cls_did_eval` might be the worst predictor, since it won't accurately reflect the proportion of students who chose to fill out, and it's confounded with `cls_students`.\n\n### Exercise 12\n\nIt turns out I was wrong. The worst $R^2$ values (and adjusted $R^2$ values) are obtained from models with `cls_profs` (the number of professors teaching the class) or `cls_students` (the number of students).\n\nThe absolute worst model is `cls_profs` (with an $R^2$ of 0.0006). In comparison, the model with `cls_did_eval` is about 6x better (with an $R^2$ of 0.004). Neither is very good at all, but my prediction/intuition was incorrect.\n\nThe coefficient for `cls_profs:single` (i.e., classes with a single professor) is negative, but the standard error is almost 2x as larger as the coefficient, suggesting a lot of uncertainty about that estimate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Fit with cls_students\nm_bty_cls_students <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_students, \n      data = evals)\n\nm_bty_cls_students |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1  0.000674      -0.00149 0.544     0.311   0.577     1  -374.  755.  767.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\n## Fit with cls_profs\nm_bty_cls_profs <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_profs, \n      data = evals)\n\nm_bty_cls_profs |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1  0.000649      -0.00152 0.544     0.299   0.585     1  -374.  755.  767.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\nm_bty_cls_profs |>\n  tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)       4.18      0.0311   134.      0    \n2 cls_profssingle  -0.0292    0.0534    -0.547   0.585\n```\n\n\n:::\n\n```{.r .cell-code}\n## Fit model with the variable I predicted would be worst\nm_bty_cls_did_eval <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ cls_did_eval, \n      data = evals)\n\nm_bty_cls_did_eval |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1   0.00395       0.00179 0.543      1.83   0.177     1  -374.  753.  766.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n### Exercise 13\n\nIf we're already going to include `cls_students` (the number of students in a class) and `cls_perc_eval` (the percentage who filled out the evaluations), we wouldn't need `cls_did_eval`, because that is simply the product of `cls_students` and `cls_perc_eval`.\n\nIncluding this parameter shouldn't improve the fit of the model, and it'll only negatively impact metrics like AIC or adjusted R-squared that take into account the number of parameters in the model.\n\n### Exercise 14\n\nHere, we fit a full model with every parameter but `cls_did_eval`.\n\nThe adjusted $R^2$ is 0.141.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_bty_full <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ bty_avg + rank +\n        ethnicity + gender + language + age +\n        cls_perc_eval + cls_students + cls_level +\n        cls_profs + cls_credits + bty_avg, \n      data = evals)\n\nm_bty_full |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.164         0.141 0.504      7.33 2.40e-12    12  -333.  694.  752.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n### Exercise 15\n\nNow, we fit a series of models, taking out one variable at a time.\n\nFor each model of size $k$, we remove one of the original parameters and ask which parameter's removal results in the highest adjusted $R^2$.\n\nIn order of removal from the full model in Exercise 15:\n\n-   `cls_profs` (yields adjusted R\\^2 of 0.143).\\\n-   `cls_level` (yields adjusted R\\^2 of 0.1448).\\\n-   `rank` (yields adjusted R\\^2 of 0.145).\n\nAfter these three, there are no other parameters for which removing them improves model fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## First reduced model, no cls_profs\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1430708\n```\n\n\n:::\n\n```{.r .cell-code}\n## Reduced model 2, no cls_level or cls_profs\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1447586\n```\n\n\n:::\n\n```{.r .cell-code}\n## Reduced model 3, no cls_level or cls_profs or rank\nlinear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        # rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals) |>\n  glance() |>\n  pull(adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1453683\n```\n\n\n:::\n:::\n\n\nThe final model is below, with the corresponding linear equation:\n\n$\\hat{y} = 3.39 + 0.062X_1 + 0.2X_2 + 0.18X_3 - 0.15X_4 - 0.005X_5 + 0.006X_6 + 0.0004X_7 + 0.52X_8$\n\nWhere:\n\n-   $X_1$ is `bty_avg`\\\n-   $X_2$ is `ethnicity` (corresponding to 1 for `not minority` professors)\\\n-   $X_3$ is `gender` (corresponding to 1 for `male` professors)\\\n-   $X_4$ is `language` (corresponding to 1 for `non-english` professors)\n-   $X_5$ is `age`\\\n-   $X_6$ is `cls_perc_eval`\\\n-   $X_7$ is `cls_students`\\\n-   $X_8$ is `cls_credits` (corresponding to 1 for `one credit` professors)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Final model.\nm_final <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(score ~ \n        bty_avg + \n        # rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        # cls_level +\n        # cls_profs +\n        cls_credits,\n      data = evals)\n\nm_final |>\n  glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.160         0.145 0.503      10.8 5.46e-14     8  -334.  688.  730.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\nm_final |>\n  tidy() |>\n  select(term, estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 2\n  term                   estimate\n  <chr>                     <dbl>\n1 (Intercept)            3.39    \n2 bty_avg                0.0619  \n3 ethnicitynot minority  0.204   \n4 gendermale             0.177   \n5 languagenon-english   -0.151   \n6 age                   -0.00487 \n7 cls_perc_eval          0.00575 \n8 cls_students           0.000407\n9 cls_creditsone credit  0.523   \n```\n\n\n:::\n:::\n\n\nA shorter non-tidy approach...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(olsrr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'olsrr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:datasets':\n\n    rivers\n```\n\n\n:::\n\n```{.r .cell-code}\n# fit model\nmod <- lm(score ~ \n        bty_avg + \n        rank +\n        ethnicity + \n        gender + \n        language + \n        age +\n        cls_perc_eval + \n        cls_students + \n        cls_level +\n        cls_profs +\n        cls_credits,\n      data = evals)\n\n# determine which predictors to eliminate\nols_step_backward_p(mod)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                           Elimination Summary                            \n-------------------------------------------------------------------------\n        Variable                   Adj.                                      \nStep     Removed     R-Square    R-Square     C(p)       AIC        RMSE     \n-------------------------------------------------------------------------\n   1    cls_profs      0.1635      0.1431    9.0279    692.3066    0.5035    \n   2    cls_level      0.1633      0.1448    7.1373    690.4192    0.5030    \n   3    rank           0.1602      0.1454    6.8068    688.1332    0.5028    \n-------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n### Exercise 16\n\nThe variable `cls_credits` encodes whether a class is worth multiple credits or simply one credit. We see there is a positive coefficient, 0.523, corresponding to the predicted difference for `one credit` classes relative to `multi credit`. That is, all else held equal, professors of `one credit` classes are predicted to have a score 0.523 higher.\n\nThe variable `cls_perc_eval` encodes what proportion of students filled out the evaluation. The coefficient is 0.00575, meaning that for every *percent* increase in how many students filled out the evaluatino, the professor is predicted to have 0.00575 increase in their `score`.\n\n### Exercise 17\n\nA high score would be associated with a professor that is:\n\n-   `male`\n-   *not* a minority\n-   perceived as more attractive\n-   younger\\\n-   received education at English-speaking school\n\nAdditionally, scores should be higher for *classes* for which:\n\n-   a higher proportion of students filled out the evaluation\\\n-   there are more students in the class\\\n-   is `one credit` (as opposed to `multi credit`)\n\n### Exercise 18\n\nThe original sample of evaluations was taken from classes/professors at University of Texas, Austin.\n\nAs always, there are constraints on generalizability. It is possible that the effects observed here are specific to either (or both) the professors or students who happen to attend UT Austin. For example, maybe at other universities, teacher evaluations are *less* impacted by perceived attractiveness of the professor---or maybe they are *more* impacted. In both cases, this parameter estimate might be a poor estimate (either over or underestimating the impact of perceived attractiveness on evaluations).\n\nOn the other hand, I don't necessarily have a strong reason *a priori* to think that UT Austin students or professors are somehow outliers in the USA higher-education system. I would feel more comfortable generalizing if I had similar results from another US university, but as it stands, I would tentatively conclude that there's evidence for the effects we observed, and suggest that the effect should be replicated in other universities.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}