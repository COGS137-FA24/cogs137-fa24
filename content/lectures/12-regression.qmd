---
title: "12-regression"
author: "Professor Shannon Ellis"
date: "2024-11-14"

format:
  html: 
    output-file: 12-regression.html
  revealjs:
    output-file: 12-regression-slides.html
    css: slides.css
---
```{r echo = FALSE, message=FALSE, warning=FALSE}
library(patchwork)

color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
```

## Q&A {.smaller}

> Q: kind of zoned out. When is it appropriate to use inference or machine learning? \
> A: We use inference when we want to quantify the relationship between features and outcome to understand relationships. We use machine learning when we want to make predictions on outcomes from features. 

> Q: I am confused why in EDA, we have been finding the correlation between variables that seem to be measuring the same thing (such as graphs for log distance to primary or primary and secondary road). Why are these helpful to us?\
> A: We are ultimately trying to build a model to predict PM2.5 values...but we have a lot of features from which to choose, many which measure very siimilar things. We're using EDA to get a sense of what data we have and the relationships between variables, before ultimately deciding what to include in our ML model.

## Course Announcements

**Due Dates**

-   `r emo::ji("science")` **Lab 06** due Thursday
-   `r emo::ji('clipboard')` Lecture Participation survey "due" after class

**Notes**

- lab05 scores posted; cs01 scores/feedback posted by EOD
- lab07 now available | longer than typical labs
- Reminder to add any CS02 EDA you want to share with your classmates to the [padlet from last class](https://padlet.com/shannon0ellis/cs02-eda-9fn2bbxjm3qfc3d3)

## Agenda

- Simple linear regression (review)
- Multiple linear regression
- Model Comparison
- Backward Selection

## Suggested Reading

- [IMS Ch7: Linear regression with a single predictor](https://openintro-ims.netlify.app/model-slr)
- [IMS Ch8: Linear regression with multiple predictors](https://openintro-ims.netlify.app/model-mlr)

# Linear Regression {background-color="#92A86A"}

## Setup
```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(olsrr) # will need to be installed
```


## Data

```{r}
pm <- read_csv("OCS_data/data/raw/pm25_data.csv")
```

## Starting Question

> What is the relationship between the EPA's CMAQ and PM2.5 values?

Reminder: Community Multiscale Air Quality (CMAQ) is the EPA's air pollution model. The data do NOT use PM2.5 measures

## Quick Look

```{r}
ggplot(pm, aes(x=CMAQ, y=value)) + 
  geom_point()
```

# Single predictor {background-color="#92A86A"}

## Regression model

$$ \hat{y} = b_0 + b_1~x_1 $$

- $\hat{y}$ | outcome
- $x_1$ | predictor
- $b_0$ | y-intercept
- $b_1$ | slope (effect size)

## Step 1: Specify model

```{r}
linear_reg()
```

## Step 2: Set model fitting *engine*

```{r}
lin_mod <- linear_reg() |>
  set_engine("lm") # lm: linear model
```

## Step 3: Fit model & estimate parameters

... using **formula syntax**

```{r}
# fit model
mod_cmaq_value <- lin_mod |>
  fit(value ~ CMAQ, data = pm) 

# display results
mod_cmaq_value |>
  tidy()
```

. . . 

$$\widehat{PM2.5}_{i} = 7.40 + 0.405 \times CMAQ_{i}$$

## Visualizing the model

```{r message=FALSE}
ggplot(data = pm, aes(x = CMAQ, y = value)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", fullrange = TRUE, color = "#8E2C90", se = FALSE)
```

## Slope and intercept

$$\widehat{PM2.5}_{i} = 7.40 + 0.405 \times CMAQ_{i}$$

. . . 

-   **Slope:** For each one unit increase in CMAQ, we expect PM2.5 to increase, on average, by 0.405 ug/m^3.

. . . 

-   **Intercept:** Monitors in areas with a CMAQ of zero are expected to have PM2.5 values of 7.4 ug/m^3, on average. (Recall: WHO exposure guideline is < 10 ug/m^3 on average annually for PM2.5)


## Model Understanding

$$\widehat{PM2.5}_{i} = 7.40 + 0.405 \times CMAQ_{i}$$

[`r emo::ji("question")` What would we expect the PM2.5 value to be in an area with a CMAQ of 2?]{style="background-color: #ADD8E6"}

## ❗️Prediction vs. extrapolation

[`r emo::ji("question")` What would we expect the PM2.5 value to be in an area with a CMAQ of 100? $$\widehat{PM2.5}_{i} = 7.40 + 0.405 \times 100$$
]{style="background-color: #ADD8E6"}

. . . 

```{r extrapolate, warning = FALSE, echo=FALSE, message=FALSE, fig.height=2.75}
newdata <- tibble(CMAQ = 100)
newdata <- newdata |>
  mutate(value = predict(mod_cmaq_value, new_data = newdata))

ggplot(data = pm, aes(x = CMAQ, y = value)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", fullrange = TRUE, color = "#8E2C90", se = FALSE) +
  xlim(0, 150) +
  ylim(0, 150) +
  geom_segment(data = newdata, mapping = aes(x = CMAQ, y = 0, xend = CMAQ, yend = value$.pred), color = color_palette$salmon, lty = 2) +
  geom_segment(data = newdata, mapping = aes(x = CMAQ, y = value$.pred, xend = 0, yend = value$.pred), color = color_palette$salmon, lty = 2)
```


## Measuring the strength of the fit

-   The strength of the fit of a linear model is most commonly evaluated using $R^2$.

-   It tells us what percent of variability in the response variable is explained by the model.

-   The remainder of the variability is explained by variables not included in the model.

-   $R^2$ is sometimes called the coefficient of determination.

## Obtaining $R^2$ in R {.smaller}

```{r}
glance(mod_cmaq_value)
glance(mod_cmaq_value)$r.squared # extract R-squared
```

Roughly 21.7% of the variability in PM2.5 values can be explained by CMAQ.

. . . 

...suggests that we can do better to explain the variance in PM2.5 values

# Multiple predictors (MLR) {background-color="#92A86A"}

## MLR 

-   Sample model that we use to estimate the population model:

$$ \hat{y} = b_0 + b_1~x_1 + b_2~x_2 + \cdots + b_k~x_k $$

## Updated question

> What is the relationship between CMAQ & population density (predictors) and  and PM2.5 values (outcome)?

-   Response variable: `value` (PM2.5)
-   Explanatory variables: `CMAQ` and Population Density hi/low

## Creating a low/high population density variable

```{r}
pm <- pm |> 
  mutate(popdens = case_when(
    popdens_zcta >= median(pm$popdens_zcta) ~ "hi",
    popdens_zcta < median(pm$popdens_zcta) ~ "low"          
  ))
```

[`r emo::ji("question")` What is this accomplishing?]{style="background-color: #ADD8E6"}

## PM2.5 & Poulation density

::: panel-tabset
### Plot

```{r ref.label = "pm25", echo = FALSE, warning = FALSE, out.width="100%"}
```


### Code

```{r pm25, fig.show = "hide"}
ggplot(data = pm, aes(x = value, fill = popdens)) +
  geom_histogram(binwidth = 2.5) + 
  facet_grid(popdens ~ .) +
  scale_fill_manual(values = c("#071381", "#E48957")) +
  guides(fill = "none") +
  labs(x = "PM2.5", y = NULL) 
```
:::

## Two ways to model

:::incremental
-   **Main effects:** Assuming relationship between CMAQ and PM2.5 **does not vary** by whether it's a low or high population density monitor.
-   **Interaction effects:** Assuming relationship between CMAQ and PM2. **varies** by whether or not it's a low or high population density monitor.
:::

## Interacting explanatory variables

-   Including an interaction effect in the model allows for different slopes, i.e. nonparallel lines.
-   This implies that the regression coefficient for an explanatory variable would change as another explanatory variable changes.
-   This can be accomplished by adding an interaction variable: the product of two explanatory variables.

## Two ways to model {.smaller}

::: columns
::: {.column width="30%"}
-   **Main effects:** Assuming relationship between CMAQ and PM2.5 **does not vary** by whether it's a low or high population density monitor.
-   **Interaction effects:** Assuming relationship between CMAQ and PM2. **varies** by whether or not it's a low or high population density monitor.
:::

::: {.column width="70%"}
```{r pp-main-int-viz, echo=FALSE, message=FALSE, out.width="800%", fig.asp = 0.9}
pm_main_fit <- lin_mod |>
  fit(value ~ CMAQ + popdens, data = pm)
pm_main_fit_aug <- augment(pm_main_fit$fit)

pm_int_fit <- lin_mod |>
  fit(value ~ CMAQ * popdens, data = pm)
pm_int_fit_aug <- augment(pm_int_fit$fit)

p_main <- ggplot(
  pm_main_fit_aug,
  aes(y = value, x = CMAQ, color = popdens)
) +
  geom_point(aes(shape = popdens), alpha = 0.5) +
  scale_color_manual(values = c("#071381", "#E48957")) +
  geom_line(aes(y = .fitted), linewidth = 1.5) +
  labs(y = "PM2.5", title = "Main effects", color = "PopDens", shape = "PopDens")

p_int <- ggplot(
  pm_int_fit_aug,
  aes(y = value, x = CMAQ, color = popdens)
) +
  geom_point(aes(shape = popdens), alpha = 0.5) +
  scale_color_manual(values = c("#071381", "#E48957")) +
  geom_line(aes(y = .fitted), linewidth = 1.5) +
  labs(y = "PM2.5", title = "Interaction effects", color = "PopDens", shape = "PopDens")

p_main /
  p_int  + 
  plot_layout(guides = "collect") & theme(legend.position = "bottom")
```
:::
:::

. . . 

[`r emo::ji("question")` Which does your intuition/knowledge of the data suggest is more appropriate?]{style="background-color: #ADD8E6"}

Put a <font color="#32cb31">green</font> sticky if you think main; <font color="#ff65a3">pink</font> if you think interaction.

## Fit model with main effects {.smaller}

```{r model-main-effects}
pm_main_fit <- lin_mod |>
  fit(value ~ CMAQ + popdens, data = pm)

pm_main_fit |> tidy()
```

. . . 

$$\widehat{PM2.5} = 7.70 + 0.389 \times CMAQ - 0.345 \times popdens$$

. . . 

[`r emo::ji("question")` How do we interpret this model?]{style="background-color: #ADD8E6"}

## Solving the model

-   High-population density: Plug in 0 for `popdens`

$$\widehat{PM2.5} = 7.70 + 0.389 \times CMAQ - 0.345 \times 0$$\
$= 7.70 + 0.389 \times CMAQ$

. . . 

-   Low-population density: Plug in 1 for `popdens`

$$\widehat{PM2.5} = 7.70 + 0.389 \times CMAQ - 0.345 \times 1$$\
$= 7.355 + 0.389 \times CMAQ$

## Visualizing main effects {.smaller}

::: columns
::: {.column width="40%"}
-   **Same slope:** *Rate of change* in PM2.5 as CMAQ increases does not vary between low- and high-population density monitor areas.
-   **Different intercept:** Areas of low density have consistently lower PM2.5 values relative to high-population density areas
:::

::: {.column width="60%"}
```{r out.width="100%", echo = FALSE}
p_main
```
:::
:::

## Interpreting main effects {.smaller}

```{r exp-coefs}
pm_main_fit |>
  tidy() |> 
  mutate(exp_estimate = exp(estimate)) |>
  select(term, estimate, exp_estimate)
```

::: incremental
-   All else held constant, for each 1 unit increase in CMAQ, PM2.5 would expect to increase by 0.389.
-   All else held constant, areas of low density have PM2.5 values, on average, that are 0.345 *lower* than in high density areas
-   PM2.5 values in high-density areas with a CMAQ of zero, would expect to have a PM2.5 value of 7.7.
:::


## Interaction: `CMAQ * popdens` {.smaller}

```{r out.width="80%", echo = FALSE}
p_int
```

## Fit model with interaction effects {.smaller}

-   Response variable: `value` (PM2.5)
-   Explanatory variables: `CMAQ`, `popdens`, and their interaction

```{r model-interaction-effects}
pm_int_fit <- lin_mod |>
  fit(value ~ CMAQ * popdens, data = pm)
```

## Linear model with interaction effects {.smaller}

```{r model-interaction-effects-tidy, echo=FALSE}
pm_int_fit |> 
  tidy()
```

$$\widehat{PM2.5} = 8.35 + 0.32 \times CMAQ - 1.45 \times popdens + 0.13 \times CMAQ * popdens$$

## Interpretation of interaction effects {.smaller}

::: incremental
-   Rate of change in PM2.5 as CMAQ increases varies depending upon PopDens (different slopes & intercepts)
:::

. . . 

::: {.column width="50%"}
```{r viz-interaction-effects2, out.width="100%", echo = FALSE}
p_int
```
:::


# Comparing models {background-color="#92A86A"}

## R-squared {.small}


-   $R^2$ is the percentage of variability in the response variable explained by the regression model.

```{r}
glance(mod_cmaq_value)$r.squared #single predictor
glance(pm_main_fit)$r.squared
glance(pm_int_fit)$r.squared
```

. . . 

-   The model with interactions has a slightly higher $R^2$.

. . . 

-   However using $R^2$ for model selection in models with multiple explanatory variables is not a good idea as $R^2$ increases when **any** variable is added to the model.

## Adjusted R-squared {.small}


Adjusted R-squared adjusts for number of terms in the model

```{r}
glance(pm_main_fit)$adj.r.squared
glance(pm_int_fit)$adj.r.squared
```

It appears that adding the interaction actually increased adjusted $R^2$, so we should indeed use the model with the interactions.


## In pursuit of Occam's razor {.smaller}

::: incremental
-   Occam's Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.

-   Model selection follows this principle.

-   We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.

-   In other words, we prefer the simplest best model, i.e. **parsimonious** model.
:::

# Backward selection {background-color="#92A86A"}

## Backward selection {.small}


::: {.small}
For this demo, we'll ignore interaction effects...and just model main effects:
:::

```{r}
pm_full <-  lin_mod |>
  fit(value ~ ., data=pm) 
```

-   $R^2$ (full): `r glance(pm_full)$adj.r.squared`

. . . 

**Remove `zcta_area`**

```{r}
pm_noarea <-  lin_mod |>
  fit(value ~ . -zcta_area, data=pm) 

glance(pm_noarea)$adj.r.squared
```

. . . 

-   $R^2$ (full): `r round(glance(pm_full)$adj.r.squared, 4)`
-   $R^2$ (no `zcta_area`): `r round(glance(pm_noarea)$adj.r.squared, 4)`

...*Increased* improved variance explained, so remove variable

. . . 

... continue to remove one by one until max variance explained is achieved. But, that process is tedious.

## Other approach: `olsrr` {.small}

```{r}
# requires package installation: 
# install.packages("olsrr")
library(olsrr)
```

. . . 

**Step 1: Fit full model (w/o `tidymodels`)**

::: {.smallest}
Note: I'm only fitting a handful of variables to demo how it works. Theoretically, you would fit the full model and compare all combinations.
:::

```{r}
# fit the model (not using tidymodels)
mod <- lm(value ~ log_pri_length_25000 + log_prisec_length_15000 +
          log_nei_2008_pm25_sum_15000 , data=pm)
```

. . . 

**Step 2: Determine which variables to remove**

```{r,  cache=TRUE}
ols_step_backward_p(mod)
```

...specifies that `log_pri_length_25000` should be removed

. . . 

**Step 2 (alternate): Compare all possible models...**

```{r, cache=TRUE}
ols_step_all_possible(mod) |>
  arrange(desc(adjr))
```

## On the full model

This will take a while to run: 

```{r, cache=TRUE, message=FALSE}
mod_full <- lm(value ~ ., data=pm)
ols_step_backward_p(mod_full)
```

. . . 

[`r emo::ji("question")` How does this maybe inform our ultimate goal (building a prediction model)?]{style="background-color: #ADD8E6"}


## Recap {.smaller background-color="#92A86A"}

-   Can you model and interpret linear models with multiple predictors?
-   Can you explain the difference in a model with main effects vs. interaction effects?
-   Can you compare different models and determine how to proceed?
-   Can you carry out and explain backward selection?



